{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/QiaoHongbo699/group_13-code/blob/main/13_code.ipynb"
      ],
      "metadata": {
        "id": "Kl6_AA8Bz6Je"
      },
      "id": "Kl6_AA8Bz6Je"
    },
    {
      "cell_type": "markdown",
      "id": "8fc7dfc4",
      "metadata": {
        "id": "8fc7dfc4"
      },
      "source": [
        "## 1. Read the HateXplain Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44267b37",
      "metadata": {
        "id": "44267b37"
      },
      "source": [
        "We first load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40703a17",
      "metadata": {
        "id": "40703a17"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/st311/hateXplain.csv')\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset information:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca806c0",
      "metadata": {
        "id": "5ca806c0"
      },
      "source": [
        "## 2. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc3463c",
      "metadata": {
        "id": "efc3463c"
      },
      "source": [
        "Process duplicate data and clean textual noise (e.g., special characters, stopwords, URLs, etc.)\n",
        "\n",
        "Deduplication: Group the data by post_id, apply majority voting to consolidate the label, and merge the target\n",
        "\n",
        "Text Cleaning: Use regular expressions to remove URLs and special characters.\n",
        "\n",
        "Stopword Removal: Eliminate common stopwords using NLTK's stopword list.\n",
        "\n",
        "Integration: Implement a clean_text function that combines these steps and apply it to the post_tokens column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e467bd8a",
      "metadata": {
        "id": "e467bd8a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b071fd",
      "metadata": {
        "id": "33b071fd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def majority_vote(labels):\n",
        "    return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "# Group by post_id and merge duplicate data\n",
        "df_grouped = df.groupby('post_id').agg({\n",
        "    'post_tokens': 'first',\n",
        "    'label': majority_vote,\n",
        "    'target': lambda x: ','.join(set(x.dropna()))\n",
        "}).reset_index()\n",
        "\n",
        "# Define the text cleaning function\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters and punctuation, keeping only letters, numbers, and spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "df_grouped['cleaned_text'] = df_grouped['post_tokens'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned text (first 5 rows)\n",
        "print(\"\\nCleaned text (first 5 rows):\")\n",
        "print(df_grouped['cleaned_text'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41275e27",
      "metadata": {
        "id": "41275e27"
      },
      "source": [
        "## 3. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c96d59",
      "metadata": {
        "id": "56c96d59"
      },
      "source": [
        "Perform two types of tokenization: BERT's WordPiece and standard tokenization for CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0cc8d3",
      "metadata": {
        "id": "1f0cc8d3"
      },
      "source": [
        "### 3.1 BERT's WordPiece Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3898338",
      "metadata": {
        "id": "a3898338"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load pretrained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define BERT tokenization function\n",
        "def bert_tokenize(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Apply BERT tokenization to cleaned text\n",
        "df_grouped['bert_input_ids'] = df_grouped['cleaned_text'].apply(lambda x: bert_tokenize(x)['input_ids'])\n",
        "df_grouped['bert_attention_mask'] = df_grouped['cleaned_text'].apply(lambda x: bert_tokenize(x)['attention_mask'])\n",
        "\n",
        "# Display BERT tokenization results\n",
        "print(\"\\nBERT Tokenization input_ids (first 5 rows):\")\n",
        "print(df_grouped['bert_input_ids'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989948eb",
      "metadata": {
        "id": "989948eb"
      },
      "source": [
        "### 3.2 Standard Tokenization for CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292ae42f",
      "metadata": {
        "id": "292ae42f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the CNN tokenizer\n",
        "tokenizer_cnn = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer_cnn.fit_on_texts(df_grouped['cleaned_text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer_cnn.texts_to_sequences(df_grouped['cleaned_text'])\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=128, padding='post', truncating='post')\n",
        "\n",
        "# Add padded sequences to the DataFrame\n",
        "df_grouped['cnn_sequences'] = list(padded_sequences)\n",
        "\n",
        "# Display CNN tokenization results\n",
        "print(\"\\nCNN Tokenization sequences (first 5 rows):\")\n",
        "print(df_grouped['cnn_sequences'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a37e6d",
      "metadata": {
        "id": "01a37e6d"
      },
      "source": [
        "## 4. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1ca4d8",
      "metadata": {
        "id": "fc1ca4d8"
      },
      "source": [
        "### 4.1 TF-IDF: Generating Feature Matrix for Traditional Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07cb4d5c",
      "metadata": {
        "id": "07cb4d5c"
      },
      "source": [
        "Calculate TF-IDF word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10746992",
      "metadata": {
        "id": "10746992"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Convert cleaned text to a TF-IDF matrix\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_grouped['cleaned_text'])\n",
        "\n",
        "# Extract labels\n",
        "y = df_grouped['label']\n",
        "print(\"TF-IDF matrix shape:\", X_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f8e2b0",
      "metadata": {
        "id": "28f8e2b0"
      },
      "source": [
        "### 4.2 GloVe (for CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e562488",
      "metadata": {
        "id": "2e562488"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "glove_path = '/content/drive/My Drive/st311/glove/glove.twitter.27B.100d.txt'\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer_cnn.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < 10000:  # Limit to max_words, consistent with Tokenizer's num_words\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Display embedding matrix shape\n",
        "print(\"\\nGloVe embedding matrix shape:\")\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2540ca98",
      "metadata": {
        "id": "2540ca98"
      },
      "source": [
        "## 5. Data Visualization (Word Frequency Analysis, Label Distribution, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62c072a",
      "metadata": {
        "id": "d62c072a"
      },
      "source": [
        "### 5.1 Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7d5130",
      "metadata": {
        "id": "fa7d5130"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a072e5",
      "metadata": {
        "id": "b4a072e5"
      },
      "source": [
        "### 5.2 Word Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e387b35d",
      "metadata": {
        "id": "e387b35d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df_grouped['cleaned_text'])\n",
        "# Convert to DataFrame and calculate word frequencies\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "word_freq_sum = word_freq.sum().sort_values(ascending=False)\n",
        "\n",
        "# Top 20 most frequent words\n",
        "plt.figure(figsize=(12, 6))\n",
        "word_freq_sum.head(20).plot(kind='bar')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb37a81",
      "metadata": {
        "id": "deb37a81"
      },
      "source": [
        "## 6. Traditional Machine Learning Baseline Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedc4f57",
      "metadata": {
        "id": "eedc4f57"
      },
      "source": [
        "### 6.2 Training and Optimizing Baseline Models (Logistic Regression and SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e9f623",
      "metadata": {
        "id": "91e9f623"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and optimize Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "param_grid_log_reg = {'C': [0.1, 1, 10]}\n",
        "grid_log_reg = GridSearchCV(log_reg, param_grid_log_reg, cv=5)\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "best_log_reg = grid_log_reg.best_estimator_\n",
        "y_pred_log_reg = best_log_reg.predict(X_test)\n",
        "\n",
        "# Train and optimize SVM\n",
        "svm_model = SVC()\n",
        "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_svm = GridSearchCV(svm_model, param_grid_svm, cv=5)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "best_svm = grid_svm.best_estimator_\n",
        "y_pred_svm = best_svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4f09db",
      "metadata": {
        "id": "de4f09db"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Logistic Regression model evaluation\n",
        "print(\"Logistic Regression model evaluation:\")\n",
        "print(\"Best parameters:\", grid_log_reg.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "# SVM model evaluation\n",
        "print(\"\\nSVM model evaluation:\")\n",
        "print(\"Best parameters:\", grid_svm.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455ee1fc",
      "metadata": {
        "id": "455ee1fc"
      },
      "source": [
        "## 7.Comparison of Cyber Violence Detection Models\n",
        "\n",
        "This Notebook integrates three scripts:\n",
        "\n",
        "- CNN + BERT mixture model\n",
        "- pure BERT baseline model\n",
        "- Auxiliary Scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7996e91",
      "metadata": {
        "id": "d7996e91"
      },
      "source": [
        "### 7.1 CNN+BERT mixture model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf8f5ed",
      "metadata": {
        "id": "7bf8f5ed"
      },
      "outputs": [],
      "source": [
        "# import library & download NLTK data\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, precision_recall_curve\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertModel,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "\n",
        "# data processing\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "# Customized Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt = self.texts.iloc[idx]\n",
        "        lbl = int(self.labels.iloc[idx])\n",
        "        enc = self.tokenizer(\n",
        "            txt,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids':     enc['input_ids'].squeeze(0),\n",
        "            'attention_mask':enc['attention_mask'].squeeze(0),\n",
        "            'label':         torch.tensor(lbl, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "#  Mixture model\n",
        "class AdaptiveAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "    def forward(self, x):\n",
        "        scores = torch.relu(self.fc1(x))       # [B, L, hidden]\n",
        "        scores = self.fc2(scores).squeeze(-1)  # [B, L]\n",
        "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)  # [B,L,1]\n",
        "        return torch.sum(weights * x, dim=1)   # [B, H]\n",
        "\n",
        "class CNNBranch(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_filters=(3,4,5), n_kernels=100, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, n_kernels, (k, embed_dim)) for k in num_filters\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out_dim = n_kernels * len(num_filters)\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids).unsqueeze(1)  # [B,1,L,EMB]\n",
        "        convs = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        pools = [torch.max(c, dim=2)[0] for c in convs]\n",
        "        cat = torch.cat(pools, dim=1)\n",
        "        return self.dropout(cat)\n",
        "\n",
        "class BERTCNNTransformerModel(nn.Module):\n",
        "    def __init__(self, num_classes, bert_path, vocab_size):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_path, local_files_only=True)\n",
        "        self.cnn  = CNNBranch(vocab_size=vocab_size,\n",
        "                              embed_dim=self.bert.config.hidden_size)\n",
        "        self.mha  = nn.MultiheadAttention(self.bert.config.hidden_size,\n",
        "                                          num_heads=8, batch_first=True)\n",
        "        self.adapt_attn = AdaptiveAttention(self.bert.config.hidden_size,\n",
        "                                            hidden_dim=256)\n",
        "        total_dim = self.cnn.out_dim + self.bert.config.hidden_size\n",
        "        self.fc = nn.Linear(total_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_out = self.bert(input_ids=input_ids,\n",
        "                             attention_mask=attention_mask)\n",
        "        last_hidden = bert_out.last_hidden_state           # [B, L, H]\n",
        "        attn_out, _ = self.mha(last_hidden, last_hidden, last_hidden,\n",
        "                              key_padding_mask=~attention_mask.bool())\n",
        "        pooled_attn = self.adapt_attn(attn_out)            # [B, H]\n",
        "        cnn_feat = self.cnn(input_ids)                     # [B, cnn_out]\n",
        "        combined = torch.cat([cnn_feat, pooled_attn], dim=1)\n",
        "        return self.fc(combined)                           # [B, num_classes]\n",
        "\n",
        "\n",
        "# visualization\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(18,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve'); plt.legend()\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Curve'); plt.legend()\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(history['train_f1'], label='Train F1')\n",
        "\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Macro-F1')\n",
        "    plt.title('F1 Curve'); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(y_true, y_prob, class_index=1):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob[:,class_index], pos_label=class_index)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "    plt.plot([0,1],[0,1],'--',color='gray')\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_prob, class_index=1):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob[:,class_index], pos_label=class_index)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.plot(recall, precision, lw=2)\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve')\n",
        "    plt.show()\n",
        "\n",
        "def plot_word_freq(texts):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    vec = CountVectorizer(stop_words='english', max_features=20)\n",
        "    X = vec.fit_transform(texts)\n",
        "    freqs = np.array(X.sum(axis=0)).flatten()\n",
        "    words = vec.get_feature_names_out()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.barh(words, freqs)\n",
        "    plt.xlabel('Frequency'); plt.title('Top 20 Words')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_dist(labels):\n",
        "    counts = labels.value_counts()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.barplot(x=counts.index, y=counts.values, palette='viridis')\n",
        "    plt.xlabel('Class'); plt.ylabel('Count'); plt.title('Class Distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Training & Validation Process\n",
        "def train_and_validate():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    BERT_PATH = r\"F:\\koutu\\nlp\\bert-base-uncased\"\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(BERT_PATH, local_files_only=True)\n",
        "\n",
        "    # data preparation\n",
        "    df = pd.read_csv(r\"C:\\Users\\Lu\\Desktop\\hateXplain.csv\")\n",
        "    df = df.dropna(subset=['post_id','label','post_tokens'])\n",
        "    df = df.groupby('post_id').agg({\n",
        "        'post_tokens':'first',\n",
        "        'label':      lambda ls: Counter(ls).most_common(1)[0][0]\n",
        "    }).reset_index()\n",
        "    df['text'] = df['post_tokens'].apply(preprocess_text)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df['label_id'] = le.fit_transform(df['label'])\n",
        "    num_labels = df['label_id'].nunique()\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df[['text','label_id']],\n",
        "        test_size=0.2,\n",
        "        stratify=df['label_id'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_ds = TextDataset(train_df['text'], train_df['label_id'], tokenizer)\n",
        "    val_ds   = TextDataset(val_df['text'],   val_df['label_id'],   tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "    model = BERTCNNTransformerModel(\n",
        "        num_classes=num_labels,\n",
        "        bert_path=BERT_PATH,\n",
        "        vocab_size=tokenizer.vocab_size\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "    EPOCHS = 10\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss':[], 'train_acc':[], 'train_f1':[],\n",
        "               'val_loss':[],   'val_acc':[],   'val_f1':[]}\n",
        "    all_preds, all_labels, all_probs = [], [], None\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        # model training\n",
        "        model.train()\n",
        "        tloss, tpreds, tlabs = 0, [], []\n",
        "        for b in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            ids  = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            lbls = b['label'].to(device)\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            loss   = criterion(logits, lbls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            tloss += loss.item()\n",
        "            preds = logits.argmax(dim=1).cpu().tolist()\n",
        "            tpreds.extend(preds)\n",
        "            tlabs.extend(lbls.cpu().tolist())\n",
        "\n",
        "        tr_loss = tloss / len(train_loader)\n",
        "        tr_acc  = accuracy_score(tlabs, tpreds)\n",
        "        tr_f1   = f1_score(tlabs, tpreds, average='macro')\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        vloss, vpreds, vlabs, vprobs = 0, [], [], []\n",
        "        with torch.no_grad():\n",
        "            for b in val_loader:\n",
        "                ids  = b['input_ids'].to(device)\n",
        "                mask = b['attention_mask'].to(device)\n",
        "                lbls = b['label'].to(device)\n",
        "\n",
        "                logits = model(ids, mask)\n",
        "                vloss += criterion(logits, lbls).item()\n",
        "\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "                preds = logits.argmax(dim=1).cpu().tolist()\n",
        "\n",
        "                vpreds.extend(preds)\n",
        "                vlabs.extend(lbls.cpu().tolist())\n",
        "                vprobs.extend(probs)\n",
        "\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = accuracy_score(vlabs, vpreds)\n",
        "        val_f1   = f1_score(vlabs, vpreds, average='macro')\n",
        "\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['train_acc'].append(tr_acc)\n",
        "        history['train_f1'].append(tr_f1)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}  \"\n",
        "              f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f}, f1={tr_f1:.4f}  \")\n",
        "\n",
        "        if epoch == EPOCHS:\n",
        "            all_preds, all_labels, all_probs = vpreds, vlabs, np.array(vprobs)\n",
        "\n",
        "    # visualization\n",
        "    plot_metrics(history)\n",
        "    plot_confusion_matrix(all_labels, all_preds, le.classes_)\n",
        "    plot_roc_curve(all_labels, all_probs, class_index=1)\n",
        "    plot_precision_recall_curve(all_labels, all_probs, class_index=1)\n",
        "    plot_word_freq(train_df['text'])\n",
        "    plot_class_dist(df['label'])\n",
        "\n",
        "    # saving model\n",
        "    torch.save(model.state_dict(), 'bert_cnn_attn_model.pth')\n",
        "    return model, tokenizer, le, device\n",
        "\n",
        "\n",
        "# Loading the final test set and predictive evaluation\n",
        "def predict_final(model, tokenizer, le, device):\n",
        "    df_final = pd.read_csv(r\"C:\\Users\\Lu\\Desktop\\final_hateXplain.csv\")\n",
        "    df_final['comment_clean'] = df_final['comment'].apply(preprocess_text)\n",
        "    df_final['label_enc']     = le.transform(df_final['label'])\n",
        "\n",
        "    ds     = TextDataset(df_final['comment_clean'],\n",
        "                         df_final['label_enc'],\n",
        "                         tokenizer)\n",
        "    loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in loader:\n",
        "            ids  = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            lbls = b['label'].to(device)\n",
        "            logits = model(ids, mask)\n",
        "            preds  = logits.argmax(dim=1).cpu().tolist()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(lbls.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1  = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(f\"\\nFinal Test — Accuracy: {acc:.4f}, Macro‑F1: {f1:.4f}\\n\")\n",
        "    print(\"Classification Report:\\n\",\n",
        "          classification_report(all_labels, all_preds, target_names=le.classes_))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title('Final Test Confusion Matrix'); plt.xlabel('Pred'); plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model, tokenizer, le, device = train_and_validate()\n",
        "    predict_final(model, tokenizer, le, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba523c93",
      "metadata": {
        "id": "ba523c93"
      },
      "source": [
        "### 7.2pure BERT baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930d1090",
      "metadata": {
        "id": "930d1090"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertModel,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt = self.texts.iloc[idx]\n",
        "        lbl = int(self.labels.iloc[idx])\n",
        "        enc = self.tokenizer(\n",
        "            txt,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(lbl, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, bert_path):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_path, local_files_only=True)\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.pooler_output\n",
        "        dropped = self.dropout(pooled)\n",
        "        logits = self.classifier(dropped)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    epochs = range(1, len(history['train_loss'])+1)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve (Baseline)'); plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, history['train_acc'], label='Train Acc')\n",
        "    plt.plot(epochs, history['val_acc'],   label='Val Acc')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Curve (Baseline)'); plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_roc_pr(y_true, y_prob, n_classes):\n",
        "    plt.figure(figsize=(12,5))\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true == i, y_prob[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'Class {i} AUC={roc_auc:.2f}')\n",
        "    plt.plot([0,1], [0,1], '--')\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "    plt.title('ROC Curve (Baseline)'); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    for i in range(n_classes):\n",
        "        precision, recall, _ = precision_recall_curve(y_true == i, y_prob[:, i])\n",
        "        plt.plot(recall, precision, lw=2, label=f'Class {i}')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve (Baseline)'); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix (Baseline)')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_and_eval(df, bert_path, batch_size=16, epochs=10, lr=2e-5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(bert_path, local_files_only=True)\n",
        "\n",
        "    df = df.dropna(subset=['post_tokens', 'label'])\n",
        "    df['text'] = df['post_tokens'].apply(preprocess_text)\n",
        "    le = LabelEncoder()\n",
        "    df['label_id'] = le.fit_transform(df['label'])\n",
        "    n_classes = len(le.classes_)\n",
        "\n",
        "    train_df, val_df = train_test_split(df[['text','label_id']], test_size=0.2, stratify=df['label_id'], random_state=42)\n",
        "    train_ds = TextDataset(train_df['text'], train_df['label_id'], tokenizer)\n",
        "    val_ds   = TextDataset(val_df['text'],   val_df['label_id'],   tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = BertClassifier(num_classes=n_classes, bert_path=bert_path).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss, preds, labs = 0., [], []\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            ids = batch['input_ids'].to(device)\n",
        "            att = batch['attention_mask'].to(device)\n",
        "            lbl = batch['label'].to(device)\n",
        "            logits = model(ids, att)\n",
        "            loss = criterion(logits, lbl)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds.extend(logits.argmax(dim=1).cpu().tolist())\n",
        "            labs.extend(lbl.cpu().tolist())\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc  = accuracy_score(labs, preds)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, v_preds, v_labs, v_probs = 0., [], [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                ids = batch['input_ids'].to(device)\n",
        "                att = batch['attention_mask'].to(device)\n",
        "                lbl = batch['label'].to(device)\n",
        "                logits = model(ids, att)\n",
        "                val_loss += criterion(logits, lbl).item()\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "                v_probs.extend(probs)\n",
        "                v_preds.extend(logits.argmax(dim=1).cpu().tolist())\n",
        "                v_labs.extend(lbl.cpu().tolist())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc  = accuracy_score(v_labs, v_preds)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} \")\n",
        "\n",
        "        if epoch == epochs:\n",
        "            plot_metrics(history)\n",
        "            plot_confusion(v_labs, v_preds, le.classes_)\n",
        "            plot_roc_pr(np.array(v_labs), np.array(v_probs), n_classes)\n",
        "            print(\"Classification Report:\\n\", classification_report(v_labs, v_preds, target_names=le.classes_))\n",
        "\n",
        "    torch.save(model.state_dict(), \"baseline_bert_model.pth\")\n",
        "    return model, tokenizer, le, device\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(r\"C:\\Users\\Lu\\Desktop\\hateXplain.csv\")\n",
        "    train_and_eval(df, bert_path=r\"F:\\koutu\\nlp\\bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606690db",
      "metadata": {
        "id": "606690db"
      },
      "source": [
        "### 7.3 Auxiliary Scripts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55ff6f6",
      "metadata": {
        "id": "c55ff6f6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"C:\\Users\\Lu\\Desktop\\hateXplain.csv\")\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset information:\")\n",
        "print(df.info())\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def majority_vote(labels):\n",
        "    return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "# Group by post_id and merge duplicate data\n",
        "df_grouped = df.groupby('post_id').agg({\n",
        "    'post_tokens': 'first',\n",
        "    'label': majority_vote,\n",
        "    'target': lambda x: ','.join(set(x.dropna()))\n",
        "}).reset_index()\n",
        "\n",
        "# Define the text cleaning function\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters and punctuation, keeping only letters, numbers, and spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "df_grouped['cleaned_text'] = df_grouped['post_tokens'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned text (first 5 rows)\n",
        "print(\"\\nCleaned text (first 5 rows):\")\n",
        "print(df_grouped['cleaned_text'].head())\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load pretrained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    r\"F:\\koutu\\nlp\\bert-base-uncased\",\n",
        "    local_files_only=True\n",
        ")\n",
        "# Define BERT tokenization function\n",
        "def bert_tokenize(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Apply BERT tokenization to cleaned text\n",
        "df_grouped['bert_input_ids'] = df_grouped['cleaned_text'].apply(lambda x: bert_tokenize(x)['input_ids'])\n",
        "df_grouped['bert_attention_mask'] = df_grouped['cleaned_text'].apply(lambda x: bert_tokenize(x)['attention_mask'])\n",
        "\n",
        "# Display BERT tokenization results\n",
        "print(\"\\nBERT Tokenization input_ids (first 5 rows):\")\n",
        "print(df_grouped['bert_input_ids'].head())\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the CNN tokenizer\n",
        "tokenizer_cnn = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer_cnn.fit_on_texts(df_grouped['cleaned_text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer_cnn.texts_to_sequences(df_grouped['cleaned_text'])\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=128, padding='post', truncating='post')\n",
        "\n",
        "# Add padded sequences to the DataFrame\n",
        "df_grouped['cnn_sequences'] = list(padded_sequences)\n",
        "\n",
        "# Display CNN tokenization results\n",
        "print(\"\\nCNN Tokenization sequences (first 5 rows):\")\n",
        "print(df_grouped['cnn_sequences'].head())\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Convert cleaned text to a TF-IDF matrix\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_grouped['cleaned_text'])\n",
        "\n",
        "# Extract labels\n",
        "y = df_grouped['label']\n",
        "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
        "import numpy as np\n",
        "\n",
        "glove_path = r\"C:\\Users\\Lu\\Desktop\\glove.twitter.27B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer_cnn.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < 10000:  # Limit to max_words, consistent with Tokenizer's num_words\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Display embedding matrix shape\n",
        "print(\"\\nGloVe embedding matrix shape:\")\n",
        "print(embedding_matrix.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df_grouped['cleaned_text'])\n",
        "# Convert to DataFrame and calculate word frequencies\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "word_freq_sum = word_freq.sum().sort_values(ascending=False)\n",
        "\n",
        "# Top 20 most frequent words\n",
        "plt.figure(figsize=(12, 6))\n",
        "word_freq_sum.head(20).plot(kind='bar')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and optimize Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "param_grid_log_reg = {'C': [0.1, 1, 10]}\n",
        "grid_log_reg = GridSearchCV(log_reg, param_grid_log_reg, cv=5)\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "best_log_reg = grid_log_reg.best_estimator_\n",
        "y_pred_log_reg = best_log_reg.predict(X_test)\n",
        "\n",
        "# Train and optimize SVM\n",
        "svm_model = SVC()\n",
        "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_svm = GridSearchCV(svm_model, param_grid_svm, cv=5)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "best_svm = grid_svm.best_estimator_\n",
        "y_pred_svm = best_svm.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Logistic Regression model evaluation\n",
        "print(\"Logistic Regression model evaluation:\")\n",
        "print(\"Best parameters:\", grid_log_reg.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "# SVM model evaluation\n",
        "print(\"\\nSVM model evaluation:\")\n",
        "print(\"Best parameters:\", grid_svm.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}